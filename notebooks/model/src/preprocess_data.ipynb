{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run /home/jovyan/work/database_operations/db_operations.ipynb import DataOperations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import Tuple, Annotated, List, Dict\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id\n",
    "from tensorflow.keras.layers import Input, Embedding, Reshape, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcess:\n",
    "    \"\"\"\n",
    "    Class for preparing dataset into model predictions\n",
    "    \"\"\"\n",
    "\n",
    "    def divide_data(self, df: DataFrame) -> Tuple[\n",
    "        Annotated[DataFrame, \"train_df\"], \n",
    "        Annotated[DataFrame, \"val_df\"],\n",
    "        Annotated[DataFrame, \"test_df\"]\n",
    "        ]:\n",
    "        \"\"\"\n",
    "        Divides data into training, validation and test datasets\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame befor spliting\n",
    "        Returns:\n",
    "            Tuple[Annotated[DataFrame, \"train_df\"], \n",
    "                  Annotated[DataFrame, \"val_df\"],\n",
    "                  Annotated[DataFrame, \"test_df\"]]:\n",
    "                - Training dataset \n",
    "                - Validation datset\n",
    "                - Testing datset\n",
    "        \"\"\"\n",
    "        try:\n",
    "            val_df, test_df = df.randomSplit([0.8, 0.2])\n",
    "            train_df, val_df = val_df.randomSplit([0.75, 0.25])\n",
    "            logging.info(\"Successfully divided data into training, validation and test datasets\")\n",
    "            return train_df, val_df, test_df\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error while dividing data into training, validation, and test datasets: {e}\")\n",
    "            raise e\n",
    "        \n",
    "    def standardize_data(self, \n",
    "                         train_df: DataFrame,\n",
    "                         val_df: DataFrame,\n",
    "                         test_df: DataFrame,\n",
    "                         targets: List[str]\n",
    "                         ) -> Tuple[DataFrame, DataFrame, DataFrame]:\n",
    "        \"\"\"\n",
    "        Standardizes all features in all datasets (training, validation, test)\n",
    "        \n",
    "        Args:\n",
    "            train_df: Training dataset\n",
    "            val_df: Validation dataset\n",
    "            test_df: Test dataset\n",
    "            targets: List of target features\n",
    "        Returns:\n",
    "            Tuple[DataFrame, DataFrame, DataFrame]:\n",
    "                - Standardized training dataset\n",
    "                - Standardized validation dataset\n",
    "                - Standardized test dataset\n",
    "        \"\"\"\n",
    "        try:\n",
    "            original_columns = train_df.columns\n",
    "            cols_to_transf = []\n",
    "            for org_col in original_columns:\n",
    "                if org_col not in targets:\n",
    "                        cols_to_transf.append(org_col)\n",
    "\n",
    "            train_targets = train_df.select(*targets).withColumn(\"index\", monotonically_increasing_id())\n",
    "            val_targets = val_df.select(*targets).withColumn(\"index\", monotonically_increasing_id())\n",
    "            test_targets = test_df.select(*targets).withColumn(\"index\", monotonically_increasing_id())\n",
    "            target_datasets = [train_targets, val_targets, test_targets]\n",
    "\n",
    "            assembler = VectorAssembler(inputCols=cols_to_transf,\n",
    "                                        outputCol=\"features\")\n",
    "            train_output = assembler.transform(train_df)\n",
    "            val_output = assembler.transform(val_df)\n",
    "            test_output = assembler.transform(test_df)\n",
    "\n",
    "            scaler = StandardScaler(inputCol=\"features\",\n",
    "                                    outputCol=\"scaled_features\",\n",
    "                                    withMean=True,\n",
    "                                    withStd=True)\n",
    "            \n",
    "            scaler_model = scaler.fit(train_output)\n",
    "            train_scaled = scaler_model.transform(train_output)\n",
    "            val_scaled = scaler_model.transform(val_output)\n",
    "            test_scaled = scaler_model.transform(test_output)\n",
    "            \n",
    "            scaled_datasets = [train_scaled, val_scaled, test_scaled]\n",
    "            final_datasets = []\n",
    "            for x, dataset in enumerate(scaled_datasets):\n",
    "                temp = dataset.select(\"scaled_features\")\n",
    "                temp = temp.rdd.map(lambda x:[float(y) for y in x[\"scaled_features\"]]).toDF(cols_to_transf)\n",
    "                temp = temp.withColumn(\"index\", monotonically_increasing_id())\n",
    "                scaled_df = temp.join(target_datasets[x], on=\"index\").drop(\"index\")\n",
    "                final_datasets.append(scaled_df)\n",
    "            \n",
    "            final_train = final_datasets[0]\n",
    "            final_val = final_datasets[1]\n",
    "            final_test = final_datasets[2]\n",
    "            logging.info(\"Successfully standardized datasets\")\n",
    "            return final_train, final_val, final_test\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error while data standardization\")\n",
    "            raise e\n",
    "\n",
    "    def embedding_categorical_data(self, \n",
    "                                   df: DataFrame, \n",
    "                                   categorical_features: List[str], \n",
    "                                   numeric_features: List[str]\n",
    "                                   ) -> Tuple[tf.Tensor, List[tf.Tensor]]:\n",
    "        \"\"\"\n",
    "        Applies embedding on categorical data and prepares inputs for neural network\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame containing a dataset\n",
    "            categorical_features: List of categorical features\n",
    "            numeric_features: List of numeric features\n",
    "        Returns:\n",
    "            Tuple[tf.Tensor, List[tf.Tensor]]:\n",
    "                - A tensor representing concatenated continuous features Inputs \n",
    "                  and embeddings for the categoircal features \n",
    "                - A List containing inputs layers for each feature\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            tf.debugging.enable_check_numerics()\n",
    "            models = []\n",
    "            inputs = []\n",
    "\n",
    "            combined_home_away_team = df.select(col(\"home_team\").alias(\"team\")) \\\n",
    "                                         .union(df.select(col(\"away_team\").alias(\"team\"))) \\\n",
    "                                         .distinct()\n",
    "            home_away_unique = combined_home_away_team.select(\"team\").count()\n",
    "\n",
    "            for cat in categorical_features:\n",
    "                if cat == \"home_team\" or cat == \"away_team\":\n",
    "                    vocab_size = home_away_unique\n",
    "                else:\n",
    "                    vocab_size = df.select(cat).distinct().count()\n",
    "\n",
    "\n",
    "                output_dim = min(50, (vocab_size // 2) + 1)\n",
    "\n",
    "                inpt = Input(shape=(1,), name=\"input_\" + cat)\n",
    "                embed = Embedding(vocab_size + 1,\n",
    "                                  output_dim,\n",
    "                                  trainable=True,\n",
    "                                  embeddings_initializer=tf.initializers.random_normal) \\\n",
    "                                  (inpt)\n",
    "\n",
    "                embed_reshaped = Reshape(target_shape=(output_dim,))(embed)\n",
    "                models.append(embed_reshaped)\n",
    "                inputs.append(inpt)\n",
    "\n",
    "            num_input = Input(shape=(len(numeric_features),), name=\"input_number_features\")\n",
    "            models.append(num_input)\n",
    "            inputs.append(num_input)\n",
    "\n",
    "            merge_models = Concatenate()(models)\n",
    "            logging.info(\"Successfully created inputs and embedding layers for model deployment\")\n",
    "            return merge_models, inputs\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error while embedding categorical features: {e}\")\n",
    "            raise e\n",
    "\n",
    "\n",
    "    def prepare_for_model(self, \n",
    "                          df: DataFrame, \n",
    "                          df_name: str,\n",
    "                          categorical_features: List[str], \n",
    "                          number_features: List[str], \n",
    "                          targets: List[str]\n",
    "                          ) -> Dict[str, Dict[str, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Prepares data for model entry\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to prepare\n",
    "            df_name: Name of data to prepare\n",
    "            categorical_features: List of categorcial features\n",
    "            number_features: List of categorcial features\n",
    "            targets: List of targets\n",
    "        Returns:\n",
    "            Dict[str, Dict[str, np.ndarray]]: \n",
    "                - A dictionary with two keys:\n",
    "                    - \"input_features\": A dictionary with every feature name as a key\n",
    "                       and each value is a numpy array containing the feature data\n",
    "                    - \"targets\": A numpy array containing targets values\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df = df.toPandas()\n",
    "            input_dict = {\n",
    "                \"input_features\": {},\n",
    "                \"targets\": {},\n",
    "                }\n",
    "            for cat in categorical_features:\n",
    "                input_dict[\"input_features\"][\"input_\" + cat] = df[cat].values\n",
    "            input_dict[\"input_features\"][\"input_number_features\"] = df[number_features].values\n",
    "            \n",
    "            for tar in targets:\n",
    "                input_dict[\"targets\"][tar] = df[tar].values\n",
    "            logging.info(f\"Successfully prepared dataset for model: {df_name}\")\n",
    "            return input_dict           \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error while preparing data for model: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
