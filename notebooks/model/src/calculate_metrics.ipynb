{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(ABC):\n",
    "    \"\"\"\n",
    "    Abstract class for calculating metrics\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def calculate_scores(self, y_true: np.ndarray, y_pred: np.ndarray):\n",
    "        \"\"\"\n",
    "        Calculatesscore for the model\n",
    "        \n",
    "        Args:\n",
    "            y_true: True labels\n",
    "            y_pred: Predicted labels\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccuracyScore(Metrics):\n",
    "    \"\"\"\n",
    "    Class for calculating accuracy score\n",
    "    \"\"\"\n",
    "    def calculate_scores(self, y_true: np.ndarray, y_pred: np.ndarray, name: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculates accuracy score\n",
    "\n",
    "        Args:\n",
    "            Name: Name of the target\n",
    "        \"\"\"\n",
    "        try:\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            # accuracy = np.round(accuracy, 2)\n",
    "            logging.info(f\"'{name}' accuracy score: {accuracy}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error while calculating accuracy: {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConfusionMatrix(Metrics):\n",
    "    \"\"\"\n",
    "    Class for calculating confusion matrix\n",
    "    \"\"\"\n",
    "\n",
    "    def calculate_scores(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculates confusion matrix\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Array containing confusion matrix values\n",
    "        \"\"\"\n",
    "        try:\n",
    "            conf_mat = confusion_matrix(y_true, y_pred)\n",
    "            return conf_mat\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error while calculating confusion matrix: {e}\")\n",
    "            raise e       \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
