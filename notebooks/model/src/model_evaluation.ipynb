{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "import logging\n",
    "from typing import Tuple, Union, Dict, Annotated\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluate:\n",
    "    \"\"\"\n",
    "    Class for model evaluation and calculating predictions\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 model: Model, \n",
    "                 test_dataset: Dict[str, Dict[str, np.ndarray]]\n",
    "                 ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: Trained tensorflow.keras model\n",
    "            test_dataset: Dictionary test dataset\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.test_labels = test_dataset[\"labels\"]\n",
    "        self.test_features = test_dataset[\"input_features\"]\n",
    "\n",
    "\n",
    "    def model_predict(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Makes predictions on a test dataset\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray]:\n",
    "                - 'home_score' predictions\n",
    "                - 'away_score' predictions\n",
    "        \"\"\"\n",
    "        try:\n",
    "            predictions = self.model.predict(self.test_features)\n",
    "            home_score_predictions, away_score_predictions = predictions\n",
    "            return home_score_predictions, away_score_predictions\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in model prediction: {e}\")\n",
    "            raise e\n",
    "        \n",
    "    def calculate_metrics(self) -> Tuple[\n",
    "        Annotated[float, \"loss\"],\n",
    "        Annotated[float, \"home_sc_loss\"],\n",
    "        Annotated[float, \"away_sc_loss\"],\n",
    "        Annotated[float, \"home_sc_rmse\"],\n",
    "        Annotated[float, \"away_sc_rmse\"]\n",
    "    ]:\n",
    "        \"\"\"\n",
    "        Calculates model loss\n",
    "\n",
    "        Returns:\n",
    "            Tuple[\n",
    "        Annotated[float, \"loss\"],\n",
    "        Annotated[float, \"home_sc_mse\"],\n",
    "        Annotated[float, \"away_sc_mse\"],\n",
    "        Annotated[float, \"home_sc_rmse\"],\n",
    "        Annotated[float, \"away_sc_rmse\"]\n",
    "            ]:\n",
    "            - loss: loss value\n",
    "            - home_sc_loss: 'home_score' loss value\n",
    "            - away_sc_loss: 'away_score' loss value\n",
    "            - home_sc_rmse: 'home_score' Root Mean Squared Error value\n",
    "            - away_sc_rmse: 'away_score' Root Mean Squared Error value\n",
    "        \"\"\"\n",
    "        try:\n",
    "            loss, home_sc_loss, away_sc_loss, home_sc_rmse, away_sc_rmse = self.model.evaluate(self.test_features,\n",
    "                                                                                             self.test_labels)\n",
    "            return loss, home_sc_loss, away_sc_loss, home_sc_rmse, away_sc_rmse\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Erorr while model evaluation: {e}\")\n",
    "            raise e "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
