{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "from typing import Tuple, Dict, Annotated\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluate:\n",
    "    \"\"\"\n",
    "    Class for model evaluation and calculating predictions\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 model: Model, \n",
    "                 test_dataset: Dict[str, Dict[str, np.ndarray]]\n",
    "                 ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: Trained tensorflow.keras model\n",
    "            test_dataset: Dictionary test dataset\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.test_targets = test_dataset[\"targets\"]\n",
    "        self.test_features = test_dataset[\"input_features\"]\n",
    "\n",
    "\n",
    "    def model_predict(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Predict targets on a test dataset\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray]:\n",
    "                - 'home_score' predictions\n",
    "                - 'away_score' predictions\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"Data prediction:\")\n",
    "            predictions = self.model.predict(self.test_features)\n",
    "            home_score_predictions, away_score_predictions = predictions\n",
    "            logging.info(\"Successfully predicted data on the model\")\n",
    "            return home_score_predictions, away_score_predictions\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in model prediction: {e}\")\n",
    "            raise e\n",
    "        \n",
    "    def calculate_metrics(self, metrics: Dict[str, str]) -> Tuple[\n",
    "        Annotated[float, \"loss\"],\n",
    "        Annotated[float, \"home_loss\"],\n",
    "        Annotated[float, \"away_loss\"],\n",
    "        Annotated[float, \"home_evaluation\"],\n",
    "        Annotated[float, \"away_evaluation\"]\n",
    "    ]:\n",
    "        \"\"\"\n",
    "        Evaluates the model\n",
    "\n",
    "        Args:\n",
    "            metrics: Dictionary with names of loss and metrics algorithms used in model\n",
    "        Returns:\n",
    "            Tuple[\n",
    "        Annotated[float, \"loss\"],\n",
    "        Annotated[float, \"home_loss\"],\n",
    "        Annotated[float, \"away_loss\"],\n",
    "        Annotated[float, \"home_evaluation\"],\n",
    "        Annotated[float, \"away_evaluation\"]\n",
    "            ]:\n",
    "            - loss: loss value\n",
    "            - home_loss: 'home_score' loss value\n",
    "            - away_loss: 'away_score' loss value\n",
    "            - home_evaluation: 'home_score' evaluation metric\n",
    "            - away_evaluation: 'away_score' evaluation metric\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"\\nCalculating metrics:\")\n",
    "            loss, home_loss, away_loss, home_evaluation, away_evaluation = self.model.evaluate(self.test_features, self.test_targets)\n",
    "            logging.info(f\"\\nLoss ({metrics['loss']}): {loss} \\n\"\n",
    "                         f\"'home_score' loss ({metrics['loss']}): {home_loss} \\n\"\n",
    "                         f\"'away_score' loss ({metrics['loss']}): {away_loss} \\n\"\n",
    "                         f\"'home_score' {metrics['metrics']}: {home_evaluation} \\n\"\n",
    "                         f\"'away_score' {metrics['metrics']}: {away_evaluation}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in model evaluation: {e}\")\n",
    "            raise e \n",
    "        \n",
    "    def round_results(self, predictions: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changes continuous into discret values\n",
    "        \n",
    "        Args:\n",
    "            predictions: Predictions with continuous values\n",
    "        Returns:\n",
    "            np.ndarray: Predictions with discret values\n",
    "        \"\"\"\n",
    "        try:\n",
    "            predictions_rounded = np.round(predictions)\n",
    "            predictions_reshaped = predictions_rounded.reshape((len(predictions_rounded)))\n",
    "            return predictions_reshaped\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error while converting continuous into discret values: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def select_one_category(self, predictions: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Selects the category with the highest probability for each row of predictions\n",
    "        \n",
    "        Args:\n",
    "            predictions: A 2D array, where each row contains the predicted probabilities for each class\n",
    "        Returns:\n",
    "            np.ndarray: An array of predicted class indicies, where each element represents \n",
    "                        the index of class with the highest probability for the correcponding row\n",
    "        \"\"\"\n",
    "        try:\n",
    "            predictions_transformed = np.argmax(predictions, axis=1)\n",
    "            return predictions_transformed\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error while celecting the category with the highest probability: {e}\")\n",
    "            raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
