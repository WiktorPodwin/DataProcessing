{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run /home/jovyan/work/operations/spark_db_connection.ipynb import create_spark_session\n",
    "%run /home/jovyan/work/operations/db_operations.ipynb import DataOperations\n",
    "%run /home/jovyan/work/etl/src/load_data.ipynb import LoadData\n",
    "%run /home/jovyan/work/etl/src/transform_data_types.ipynb import ResultsDataType\n",
    "%run /home/jovyan/work/etl/src/data_extraction.ipynb import DataExtraction\n",
    "%run /home/jovyan/work/model/src/transform_data.ipynb import TransformData\n",
    "%run /home/jovyan/work/model/src/preprocess_data.ipynb import PreProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Annotated, Tuple, List\n",
    "from pyspark.sql import SparkSession, DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETL:\n",
    "    \"\"\"\n",
    "    Class for ETL process (extract -> transform -> load)\n",
    "    \"\"\"\n",
    "        \n",
    "    def extract_data(self, \n",
    "                     spark: SparkSession, \n",
    "                     path: str = \"/home/jovyan/work/dataset/results.csv\", \n",
    "                     table_name: str = \"results\"\n",
    "                     ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Extracts the data\n",
    "\n",
    "        Args:\n",
    "            spark: Active SparkSession\n",
    "            path: Path to the CSV file storing data\n",
    "            table_name: Name for the table to store data in database\n",
    "        Returns:\n",
    "            DataFrame: Extracted data from database\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data_extraction = DataExtraction(spark)\n",
    "            data_extraction.save_to_database(path=path, \n",
    "                                             table_name=table_name)\n",
    "            load_data = LoadData(spark)\n",
    "            df = load_data.load_from_database(table_name=table_name)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error while extracting data: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def transform_data(self, df: DataFrame) -> Tuple[\n",
    "        Annotated[DataFrame, \"cleaned_df\"],\n",
    "        Annotated[DataFrame, \"train\"],\n",
    "        Annotated[DataFrame, \"val\"],\n",
    "        Annotated[DataFrame, \"test\"]]:\n",
    "        \"\"\"\n",
    "        Transforms the data\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to transformation\n",
    "        Returns:\n",
    "            Tuple[\n",
    "            Annotated[DataFrame, \"cleaned_df\"],\n",
    "            Annotated[DataFrame, \"train\"],\n",
    "            Annotated[DataFrame, \"val\"],\n",
    "            Annotated[DataFrame, \"test\"]]:\n",
    "                - cleaned_df: Cleaned and transformed DataFrame\n",
    "                - train: Training dataset\n",
    "                - val: Validation dataset\n",
    "                - test: Test dataset\n",
    "        \"\"\"\n",
    "        try:\n",
    "            results_dtype = ResultsDataType()\n",
    "            correct_dtypes = results_dtype.define_dtype(df)\n",
    "\n",
    "            transform_data = TransformData()\n",
    "            deleted_empty_fields = transform_data.check_empty_fields(correct_dtypes)\n",
    "            date_into_years = transform_data.convert_date_into_years(deleted_empty_fields)\n",
    "            filtered_data = transform_data.filter_data(date_into_years)\n",
    "            cat_feat, num_feat, targ = transform_data.describe_features_types()\n",
    "            cleaned_df = transform_data.string_into_numeric(df=filtered_data,\n",
    "                                                            categorical_features=cat_feat,\n",
    "                                                            numeric_features=num_feat,\n",
    "                                                            targets=targ)\n",
    "            preprocess_data = PreProcess()\n",
    "            train_df, val_df, test_df = preprocess_data.divide_data(cleaned_df)\n",
    "            train, val, test = preprocess_data.standardize_data(train_df=train_df, \n",
    "                                                                 val_df=val_df, \n",
    "                                                                 test_df=test_df,\n",
    "                                                                 targets=targ)\n",
    "            \n",
    "            return cleaned_df, train, val, test\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in data transformation: {e}\")\n",
    "            raise e\n",
    "        \n",
    "    def load_data(self, \n",
    "                  spark: SparkSession, \n",
    "                  df: DataFrame,\n",
    "                  table_name: str = \"cleaned_data\"\n",
    "                  ) -> None:\n",
    "        \"\"\"\n",
    "        Saves cleaned and transformed data into database\n",
    "        \n",
    "        Args:\n",
    "            spark: Active SparkSession\n",
    "            df: Data to load\n",
    "            table_name: Name of table to store the data\n",
    "        Returns:\n",
    "            table_name: Name of table to store the data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            db_operations = DataOperations(spark)\n",
    "            db_operations.save_data(df=df, \n",
    "                                    table_name=table_name)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in data loading: {e}\")\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_pipeline(spark: SparkSession,\n",
    "                 data_path: str = \"/home/jovyan/work/dataset/results.csv\", \n",
    "                 table_name: str = \"results\") -> None:\n",
    "    \"\"\"\n",
    "    Executes a pipeline for ETL process\n",
    "\n",
    "    This pipeline performs the following steps:\n",
    "    1. **Data Extraction**: Connects to a PostgreSQL database and saves raw data.\n",
    "    2. **Data Transformation**: Transforms and prepares the data for model training, including defining schemas.\n",
    "    3. **Data Loading**: Loads the data to the database.\n",
    "\n",
    "    Args:\n",
    "        spark: Active SparkSession\n",
    "        data_path: Path to the CSV file connecting raw data to extract\n",
    "        table_name: Name of the table in PostgreSQL databse, where the data is stored\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(\"Started ETL process\")\n",
    "        etl = ETL()\n",
    "        df = etl.extract_data(spark, data_path, table_name)\n",
    "        cleaned_df, train, val, test = etl.transform_data(df)\n",
    "        etl.load_data(spark, cleaned_df)\n",
    "        etl.load_data(spark, train, \"train\")\n",
    "        etl.load_data(spark, val, \"val\")\n",
    "        etl.load_data(spark, test, \"test\")\n",
    "        logging.info(\"Successfully finished ETL process \\n\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in ETL pipeline: {e}\")\n",
    "        raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
